{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Cyclical LR policy\n",
    "- Modified: Oct 26, 2019\n",
    "\n",
    "This notebook experiements with the cyclical leraning rate policy suggested by [Smith2017](https://arxiv.org/abs/1506.01186)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "import pdb, typing\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import xarray as xr\n",
    "\n",
    "from holoviews import opts\n",
    "from holoviews.operation.datashader import datashade, shade, dynspread, rasterize\n",
    "from holoviews.streams import Stream, param\n",
    "from holoviews import streams\n",
    "\n",
    "hv.notebook_extension('bokeh')\n",
    "hv.Dimension.type_formatters[np.datetime64] = '%Y-%m-%d'\n",
    "\n",
    "# Dashboards\n",
    "import param as pm, panel as pn\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Geoviews visualization default options\n",
    "H,W, = 250,250\n",
    "opts.defaults(\n",
    "    opts.RGB(height=H, width=W, tools=['hover'], active_tools=['wheel_zoom']),\n",
    "    opts.Image(height=H, width=W, tools=['hover'], active_tools=['wheel_zoom'], framewise=True),#axiswise=True ),\n",
    "    opts.Points( tools=['hover'], active_tools=['wheel_zoom']),\n",
    "    opts.Curve( tools=['hover'], active_tools=['wheel_zoom'], padding=0.1),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SCRIPTS = ROOT/'codes'\n",
    "paths2add = [this_nb_path, SCRIPTS]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print(\"this nb path: \", str(this_nb_path))\n",
    "print('Scripts folder: ', str(SCRIPTS))\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(str(p), \"added to the path\\n\")\n",
    "        \n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "def f(x):\n",
    "    return x\n",
    "\n",
    "interact(f, x=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class TriangleLR():\n",
    "    def __init__(self, min_lr:float, max_lr:float, stepsize:int):\n",
    "        \"\"\"\n",
    "        min_lr (float): lower bound of the learning rate\n",
    "        max_lr (float): upper bound of the lr\n",
    "        stepsize (int): stepsize in number of iterations\n",
    "            - 2*stepsize = cycle_length in iterations\n",
    "        \"\"\"\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.stepsize = stepsize\n",
    "    \n",
    "    def __call__(self, x:int):\n",
    "        \"\"\"\n",
    "        x (int): iteration index \n",
    "        \"\"\"\n",
    "        x = x%(2*self.stepsize)\n",
    "        slope = (self.min_lr - self.max_lr)/self.stepsize\n",
    "        return slope * abs(x - self.stepsize) + self.max_lr\n",
    "    \n",
    "    def step(self):\n",
    "        pass\n",
    "    \n",
    "class ConstLR():\n",
    "    def __init__(self, lr):\n",
    "        \"\"\"\n",
    "        Returns a constant LR\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "    \n",
    "    def __call__(self, x:int):\n",
    "        \"\"\"\n",
    "        x (int): iteration index \n",
    "        \"\"\"\n",
    "        return self.lr\n",
    "    \n",
    "    def step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "min_lr = 1e-3\n",
    "max_lr = 1.\n",
    "stepsize = 10\n",
    "TLR = TriangleLR(min_lr, max_lr, stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "TLR(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "xs = np.arange(100)\n",
    "ys = [TLR(x) for x in xs]\n",
    "hv.Curve((xs, ys)).opts(width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Set random seed\n",
    "- https://github.com/pytorch/pytorch/issues/7068#issuecomment-484918113\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "def random_seed(seed_value, use_cuda):\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    random.seed(seed_value) # Python\n",
    "    if use_cuda: \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# check\n",
    "for i in range(3):\n",
    "    random_seed(0, False)\n",
    "    lin = nn.Linear(1,1); \n",
    "    print(lin.weight.data.item(), lin.bias.data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Find a reasonable bound of learning rates\n",
    " \n",
    "> \"There is a simple way to estimate reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. This test is enormously valuable whenever you are facing a new architecture or dataset\" (Smith2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```python\n",
    "c = 0\n",
    "while c < 15:\n",
    "    for i, (x,y) in enumerate(dl):\n",
    "        if c >= 15:\n",
    "            break\n",
    "        print(i, x,y)\n",
    "        c += 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def found_soln(model, true_w, true_b, threshold=0.1):\n",
    "    return np.abs(model.weight.data.item()-true_w)/true_w < threshold and \\\n",
    "        np.abs(model.bias.data.item() - true_b)/true_b < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def lr_range_test(model, dataloader, optimizer, loss_fn,\n",
    "                  lr_gen, n_iters, print_every=None):\n",
    "    model.train()\n",
    "    accs = []\n",
    "    losses = []\n",
    "    count = 0\n",
    "    while True:\n",
    "        for x,y in dataloader:\n",
    "            if count >= n_iters: #or found_soln(model, true_w, true_b):\n",
    "                return count, accs, losses\n",
    "            x.unsqueeze_(-1)\n",
    "            y.unsqueeze_(-1)\n",
    "            \n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            losses.append(loss.item())\n",
    "#             print('x: ',x.shape, x)\n",
    "#             print('y: ', y.shape, y)\n",
    "#             print('loss: ', losses[-1])\n",
    "#             acc = compute_acc(pred, y)\n",
    "#             accs.append(acc)\n",
    "\n",
    "            # backprop (i.e. update the weights)\n",
    "            lr = lr_gen(count)\n",
    "            optimizer.param_groups[0]['lr'] = lr\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "\n",
    "            if print_every and count%print_every == 0:\n",
    "                print(f'\\nIter: {count}')\n",
    "                print(f'lr:  {lr}')\n",
    "                print(f\"dW: {model.weight.grad.data}\")\n",
    "                print(f\"db: {model.bias.grad.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Simple 1d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class TableDS(Dataset):\n",
    "    \n",
    "    def __init__(self, xs, ys):\n",
    "        super().__init__()\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "#         return np.array(self.xs[i]).reshape((1,-1)), np.array(self.ys[i])\n",
    "        return np.array(self.xs[i]), np.array(self.ys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 10, 500, dtype=np.float32)\n",
    "true_w, true_b = 2.5, 1.0\n",
    "ys = (true_w*xs + true_b + np.random.rand(len(xs))).astype(np.float32)\n",
    "hv.Curve((xs, ys)).opts(width=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ds = TableDS(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "dl = DataLoader(ds, batch_size=2)\n",
    "print(f'Iterations in an epoch: {len(dl)}')\n",
    "x,y = next(iter(dl))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def get_clean_model(set_seed=None):\n",
    "    if set_seed is not None:\n",
    "        random_seed(set_seed, use_cuda=False)\n",
    "\n",
    "    model = nn.Linear(1, 1)\n",
    "    print(f'Initial weights : {model.weight.data, model.bias.data}')\n",
    "    return model\n",
    "\n",
    "def show_lr_generator(lr_gen, n_iters):\n",
    "    xs = np.arange(n_iters)\n",
    "    ys = [lr_gen(x) for x in xs]\n",
    "    return hv.Curve((xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = get_clean_model(seed)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# lr schedule\n",
    "n_cycles = 4 # range-test iteration counts in cycle\n",
    "n_iters = n_cycles * len(dl)\n",
    "max_lr = 0.0001\n",
    "divide_factor = 3.\n",
    "min_lr = max_lr/divide_factor\n",
    "stepsize = 1* len(dl) # 2epochs in iteration unit\n",
    "\n",
    "tri_lr = TriangleLR(min_lr, max_lr, stepsize)\n",
    "const_lr = ConstLR(min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ii, _, loss_tri_lr = lr_range_test(model, dl, optimizer, loss_fn, \n",
    "                                 tri_lr, n_iters, print_every=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# compare with constant learning rate scheuler (with lr at min_lr)\n",
    "model = get_clean_model(seed)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "ii, _, loss_const_lr = lr_range_test(model, dl, optimizer, loss_fn, \n",
    "                                 const_lr, n_iters, print_every=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "hv_lr = show_lr_generator(tri_lr)\n",
    "\n",
    "layout = (\n",
    "    hv_lr.opts(color='red', ylim=(min_lr, max_lr)) +\n",
    "    hv.Curve(loss_tri_lr).opts(color='blue') * hv.Curve(loss_const_lr).opts(color='green')\n",
    ")\n",
    "\n",
    "layout.opts(\n",
    "    opts.Overlay(shared_axes=False),\n",
    "    opts.Curve(padding=0.1, width=800, axiswise=True,shared_axes=False)\n",
    ").cols(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print('Number of iterations: ', ii)\n",
    "print('Trained weight and bias: ', model.weight.data.item(), model.bias.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print('Difference ratio for weight: ', abs(model.weight.data.item()-true_w)/true_w)\n",
    "print('Difference ratio for bias: ', abs(model.bias.data.item()-true_b)/true_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Interesting that bias is harder to learn (or, takes longer to learn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- Experiement 2\n",
    "Set `stepsize` to 1/2 epach. Equivalent to setting `cycle_len` to one epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = get_clean_model(seed)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_cycles = 4 # range-test iteration counts in cycle\n",
    "n_iters = n_cycles * len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# max_lr = 0.0001\n",
    "# divide_factor = 3.\n",
    "# min_lr = max_lr/divide_factor\n",
    "stepsize = int(0.5* len(dl)) # 2epochs in iteration unit\n",
    "tri_lr2 = TriangleLR(min_lr, max_lr, stepsize)\n",
    "const_lr = ConstLR(max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ii, _, loss_tri_lr2 = lr_range_test(model, dl, optimizer, loss_fn, \n",
    "                                 tri_lr2, n_iters, print_every=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "hv_lr = show_lr_generator(tri_lr2, n_iters)\n",
    "\n",
    "layout = (\n",
    "    hv_lr.opts(color='red', ylim=(min_lr, max_lr)) +\n",
    "    (\n",
    "        hv.Curve(loss_const_lr).opts(color='black',line_alpha=0.5, line_width=0.3) *\n",
    "        hv.Curve(loss_tri_lr).opts(color='blue') * \n",
    "        hv.Curve(loss_tri_lr2).opts(color='green')\n",
    "    )\n",
    ")\n",
    "\n",
    "layout.opts(\n",
    "    opts.Overlay(shared_axes=False),\n",
    "    opts.Curve(padding=0.1, width=1000, axiswise=True,shared_axes=False)\n",
    ").cols(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![const-lr-vs-tri-lr](../assets/const-lr-vs-cyclic-lr.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print('Number of iterations: ', ii)\n",
    "print('Trained weight and bias: ', model.weight.data.item(), model.bias.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print('Difference ratio for weight: ', abs(model.weight.data.item()-true_w)/true_w)\n",
    "print('Difference ratio for bias: ', abs(model.bias.data.item()-true_b)/true_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- Experiement 3: even smaller stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = get_clean_model(seed)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# n_cycles = 4 # range-test iteration counts in cycle\n",
    "# n_iters = n_cycles * len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# max_lr = 0.0001\n",
    "# divide_factor = 3.\n",
    "# min_lr = max_lr/divide_factor\n",
    "stepsize = int( (1/4)*len(dl)) # 2epochs in iteration unit\n",
    "tri_lr3 = TriangleLR(min_lr, max_lr, stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ii, _, loss_tri_lr3 = lr_range_test(model, dl, optimizer, loss_fn, \n",
    "                                 tri_lr3, n_iters, print_every=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "hv_lr = show_lr_generator(tri_lr3)\n",
    "\n",
    "layout = (\n",
    "    hv_lr.opts(color='red', ylim=(min_lr, max_lr)) +\n",
    "    (\n",
    "        hv.Curve(loss_const_lr).opts(color='black',line_alpha=0.5, line_width=0.3) *\n",
    "        hv.Curve(loss_tri_lr).opts(color='blue') * \n",
    "        hv.Curve(loss_tri_lr2).opts(color='green') *\n",
    "        hv.Curve(loss_tri_lr3).opts(color='yellow')\n",
    "    )\n",
    ")\n",
    "\n",
    "layout.opts(\n",
    "    opts.Overlay(shared_axes=False),\n",
    "    opts.Curve(padding=0.1, width=1000, axiswise=True,shared_axes=False)\n",
    ").cols(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print('Number of iterations: ', ii)\n",
    "print('Trained weight and bias: ', model.weight.data.item(), model.bias.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print('Difference ratio for weight: ', abs(model.weight.data.item()-true_w)/true_w)\n",
    "print('Difference ratio for bias: ', abs(model.bias.data.item()-true_b)/true_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- Let's make a function to run experiements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def run_experiment(n_iters, dl, lr_scheduler, seed=1, to_show=True):\n",
    "    \"\"\"\n",
    "    Model architecture is fixed. One unit linear layer, ie. linear regression problem\n",
    "    \n",
    "    Args:\n",
    "    - seed (None or int): random seed for clean model (model weights)\n",
    "        - None if randomness in initializationg model weights is desired\n",
    "        - any other int to set the seed\n",
    "        \n",
    "    - lr_scheduler: TriangleLR or ConstLR\n",
    "    \n",
    "    \"\"\"\n",
    "    model = get_clean_model(seed)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0001) #this lr will be always overridden\n",
    "    loss_fn = nn.MSELoss()\n",
    "    iter_count, _, loss_lr= lr_range_test(model, dl, optimizer, loss_fn, lr_scheduler, n_iters);\n",
    "    \n",
    "    # Visualization\n",
    "    if to_show:\n",
    "        hv_lr = show_lr_generator(lr_scheduler, n_iters)\n",
    "\n",
    "        layout = (\n",
    "            hv_lr.opts(color='red', ylim=(lr_scheduler.min_lr, lr_scheduler.max_lr)) +\n",
    "            hv.Curve(loss_lr).opts(color='blue') \n",
    "        )\n",
    "\n",
    "        display(\n",
    "            layout.opts(\n",
    "            opts.Overlay(shared_axes=False),\n",
    "            opts.Curve(padding=0.1, width=800, axiswise=True,shared_axes=False)\n",
    "            ).cols(1)\n",
    "        )\n",
    "    \n",
    "    return loss_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=1)\n",
    "run_experiment(n_iters, dl, tri_lr, seed=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dl_2 = DataLoader(ds, batch_size=2); print('epoch size: ', len(dl_2))\n",
    "run_experiment(n_iters, dl_16, tri_lr, seed=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dl_2 = DataLoader(ds, batch_size=2); print('epoch size: ', len(dl_2))\n",
    "tri_lr_2 = TriangleLR(2*min_lr, 2*max_lr, stepsize)\n",
    "run_experiment(n_iters, dl_2, tri_lr_2, seed=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Summary\n",
    "\n",
    "Syncing the period of cyclic learning rate scheduler with the epoch (ie. how many iterations in an epoch) can be a good indicator on whether a single epoch is good enough to learn the mapping. Here, as we see, it is enough -- linear regression is a fairly easy task to be solved with stochastic gradient descent. \n",
    "\n",
    "More things to try\n",
    "- what about if we set stepsize = 1/2 ep?\n",
    "- what about if we change the batch size? \n",
    "    - rule of thumb is to increase lr as we increate batchsize.\n",
    "    - there is also a paper saying the raio of lr:batchsize is important for SGD to converge to a flatter minima\n",
    "        - it was a paper with Bengio, saw it on SOF comment [here]()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## TODO: \n",
    "Modified: Oct 27, 2019\n",
    "\n",
    "\n",
    "Summarize the effect of `stepsize` wrt the epoch length\n",
    "- here one epoch = 500 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Experiment with `max_lr`\n",
    "- What is I increate the max_lr to 1e-3?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "print(f'Initial weights : {model.weight.data, model.bias.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_cycles = 1 # range-test iteration counts in cycle\n",
    "n_iters = n_cycles * len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "max_lr = 0.01\n",
    "divide_factor = 3.\n",
    "min_lr = max_lr/divide_factor\n",
    "stepsize = 2 * len(dl) # 2epochs in iteration unit\n",
    "const_lr = ConstLR(max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ii_2, _, loss_list2 = lr_range_test(model, dl, optimizer, loss_fn, const_lr, n_iters, print_every=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%opts Curve [shared_axes=False]\n",
    "hv.Curve(loss_list).opts(width=1000) * hv.Curve(loss_list2).opts(width=1000, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "^ red: constant learning rate, blue: cyclic (triangle) learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ii_2, model.weight.data, model.bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Demo videos\n",
    "- https://recordit.co/97dS4ayylC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Learning rate and batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hyperparameters in training a neural network are coupled with each other. If I change one parameter (Eg. batch size), then other parameters (eg. learning rate) needs to be adjusted. Another example is that if I change the learning rate of a momentum-based optimizer (eg. Adam), it's recommended to adjust the momentum accordingly.  \n",
    "\n",
    "- Decrease the momentum as learning rate increases following the triangle learning rate (linear increase, linear decrease) schedule: the intuition is that since we are continuously increasing the learning rate, we don't want the momemtum to even push the learning rate further up (or down, when coming down)\n",
    "    \n",
    "- Reference: \n",
    "    - fastai implementation \n",
    "    - [SOF]https://stackoverflow.com/a/55690257)\n",
    "\n",
    "Let's see how learning rate and batch size affect each other in the simple 1d linear regression (ie. a `nn.Linear` layer with a single output unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bs = 10\n",
    "dl = DataLoader(ds, batch_size=bs, num_workers=0)\n",
    "print(f'Iterations in an epoch: {len(dl)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = get_clean_model(set_seed=seed)\n",
    "print(f'Initial weights : {model.weight.data, model.bias.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_cycles = 3 # range-test iteration counts in cycle\n",
    "n_iters = n_cycles * len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "max_lr = 0.01\n",
    "divide_factor = 3.\n",
    "min_lr = max_lr/divide_factor\n",
    "stepsize = 2 * len(dl) # 2epochs in iteration unit\n",
    "triangle_lr = TriangleLR(min_lr, max_lr, stepsize)\n",
    "\n",
    "tri_lr = TriangleLR(min_lr, max_lr, stepsize)\n",
    "print('batch size: ', dl.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "iter_count , _ , l3 = lr_range_test(model, dl, optimizer, loss_fn, tri_lr, n_iters, print_every=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:earthml]",
   "language": "python",
   "name": "conda-env-earthml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
